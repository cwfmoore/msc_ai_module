[dataset_features]
continuous_features = [
    "previous_qualification_grade",
    "admission_grade", 
    "age_at_enrollment",
    "curricular_units_1st_sem_grade",
    "curricular_units_2nd_sem_grade",
    "unemployment_rate",
    "inflation_rate", 
    "gdp",
    "curricular_units_1st_sem_credited",
    "curricular_units_1st_sem_enrolled",
    "curricular_units_1st_sem_evaluations",
    "curricular_units_1st_sem_approved",
    "curricular_units_1st_sem_without_evaluations",
    "curricular_units_2nd_sem_credited",
    "curricular_units_2nd_sem_enrolled",
    "curricular_units_2nd_sem_evaluations",
    "curricular_units_2nd_sem_approved",
    "curricular_units_2nd_sem_without_evaluations"
]
categorical_features = [
    "marital_status",
    "application_mode", 
    "application_order",
    "course",
    "daytime_evening_attendance",
    "previous_qualification",
    "nationality",
    "mothers_qualification",
    "fathers_qualification", 
    "mothers_occupation",
    "fathers_occupation",
    "displaced",
    "educational_special_needs",
    "debtor",
    "tuition_fees_up_to_date",
    "gender",
    "scholarship_holder",
    "international"
]

[k-NN_model]
scaler_type = "minmax" # Options: "standard", "minmax"
train_size = 0.8 # Proportion of the dataset to include in the train split
stratify = true # Whether to stratify the train-test split
n_neighbors = 5 # Set to false will result in the search for the best number of neighbors
n_neighbors_range = [3, 10] # Range of neighbors to search for the best one
cv = 10 # Number of folds for cross-validation
scoring = "f1"

[logistic_regression_model]
scaler_type = "standard"  # Options: "standard", "minmax"
train_size = 0.8  # Proportion of the dataset to include in the train split
use_acceleration = true  # Enable Intel acceleration for faster training

[pytorch_logistic_regression_testing]
# Ultra quick - 1 combination
lr_values = [0.01]
max_epochs_values = [50]
weight_decay_values = [0]
batch_size_values = [64]
optimizer_types = ["adam"]
cv_folds = 2
scoring_metric = "f1"
random_state = 42

[pytorch_logistic_regression_quick]
# Quick exploration - ~96 combinations
lr_values = [0.001, 0.01, 0.1]
max_epochs_values = [50, 100]
weight_decay_values = [0, 0.001, 0.01]
batch_size_values = [32, 64]
optimizer_types = ["adam", "sgd"]
cv_folds = 3  # Faster validation
scoring_metric = "f1"
random_state = 42

[pytorch_logistic_regression_detailed]
# Detailed search around best quick results - ~500 combinations
lr_values = [0.005, 0.01, 0.02, 0.05, 0.1]
max_epochs_values = [75, 100, 150, 200]
weight_decay_values = [0, 0.0005, 0.001, 0.005, 0.01]
batch_size_values = [32, 64, 128]
optimizer_types = ["adam", "adamw"]
cv_folds = 5
scoring_metric = "f1"
random_state = 42

# ------------------------------------------------------

[pytorch_logistic_regression_intensive]

# Learning rates - logarithmic scale for thorough exploration
lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]

# Training epochs - early, standard, and extended training
max_epochs_values = [25, 50, 100, 150, 200, 300]

# L2 regularisation - from none to heavy regularisation
weight_decay_values = [0, 0.00001, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]

# Batch sizes - from small (more updates) to large (stable gradients)
batch_size_values = [16, 32, 64, 128, 256]

# Optimizers with their specific parameters
optimizer_types = ["adam", "sgd", "adamw"]

# SGD momentum values (only used when optimizer is SGD)
sgd_momentum_values = [0.0, 0.5, 0.9, 0.95]

# Adam beta parameters (only used for Adam/AdamW)
adam_beta1_values = [0.9, 0.95]
adam_beta2_values = [0.999, 0.9999]

# Learning rate scheduling
use_lr_scheduler = true
lr_scheduler_types = ["none", "step", "exponential"]
lr_scheduler_step_size = [30, 50, 100]  # For StepLR
lr_scheduler_gamma = [0.1, 0.5, 0.8]    # Decay factor

# Dropout for regularisation (if you want to add to your model)
dropout_rates = [0.0, 0.1, 0.2, 0.3]

# Cross-validation
cv_folds = 5
scoring_metric = "f1"
random_state = 42

# Performance settings
max_combinations = 200  # Limit grid search if too many combinations
early_stopping_patience = 10  # Stop if no improvement for N epochs
