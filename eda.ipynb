{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d935c2",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdc612",
   "metadata": {},
   "source": [
    "## 1.A Summary\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Initial Setup and Data Loading**</span>\n",
    "\n",
    "This notebook establishes the foundation for analysing student dropout prediction using machine learning techniques.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Key Components**</span>\n",
    "- **Configuration Management**: Loads settings from `config.toml` using `tomllib`\n",
    "- **Data Import**: Imports essential libraries for data analysis (`pandas`, `numpy`), visualisation (`matplotlib`, `seaborn`), and statistical analysis (`statsmodels`)\n",
    "- **Dataset Loading**: Loads the \"Predict Students' Dropout and Academic Success\" dataset from UCI ML Repository\n",
    "- **Data Source**: CSV file with semicolon delimiter containing student demographic and academic performance data\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Dataset Reference:**</span>\n",
    "Realinho, V., Martins, M.V., Machado, J. and Baptista, L.M.T., 2021. *Predict Students' Dropout and Academic Success*. UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9fbf9",
   "metadata": {},
   "source": [
    "## 1.B Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from rich import print\n",
    "import pandas as pd\n",
    "from tools import Tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import pointbiserialr\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80320a",
   "metadata": {},
   "source": [
    "## 1.B Invoke Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke classes\n",
    "tools = Tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe4658",
   "metadata": {},
   "source": [
    "## 1.C Load Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(\"config.toml\", \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "    tools.print_message('success', 'Loaded configuration', format_dict={'number of keys': len(config)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0132b",
   "metadata": {},
   "source": [
    "## 1.D Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "# Realinho, V., Martins, M.V., Machado, J. and Baptista, L.M.T., 2021. Predict Students' Dropout and Academic Success. UCI Machine Learning Repository. Available at: https://doi.org/10.24432/C5MC89 [Accessed 31 May 2025].\n",
    "df_dataset = tools.load_dataset(file_name='dataset_raw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e748b",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd52a5",
   "metadata": {},
   "source": [
    "## 2.A Dataset Overview\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Dataset Characteristics**</span>\n",
    "\n",
    "**Source**: UCI Machine Learning Repository (DOI: 10.24432/C5MC89)  \n",
    "**Institution**: Instituto Politécnico de Portalegre, Portugal\n",
    "\n",
    "- **Dimensions**: 4,424 students × 37 features\n",
    "- **Data Quality**: Complete dataset (no missing values)\n",
    "- **Target**: 3-class classification (Dropout, Enrolled, Graduate)\n",
    "- **Purpose**: Early identification of at-risk students for targeted intervention\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Feature Distribution**</span>\n",
    "\n",
    "**Continuous Features (18)**:\n",
    "- **Academic Performance**: Previous qualification grade, admission grade, semester grades\n",
    "- **Curricular Progress**: Units credited, enrolled, evaluated, approved (both semesters)\n",
    "- **Demographics**: Age at enrollment\n",
    "- **Economic Context**: Unemployment rate, inflation rate, GDP\n",
    "\n",
    "**Categorical Features (18)**:\n",
    "- **Personal**: Marital status, gender, nationality, displaced status\n",
    "- **Academic**: Course, attendance mode, previous qualification level, special needs\n",
    "- **Family Background**: Parents' qualifications and occupations\n",
    "- **Financial**: Scholarship status, debtor status, tuition payment status\n",
    "- **Admission**: Application mode and order\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Data Types**</span>\n",
    "- **Integer (29 columns)**: Primarily categorical variables and count data\n",
    "- **Float (7 columns)**: Grades, rates, and continuous measurements  \n",
    "- **Object (1 column)**: Target variable\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Research Context**</span>\n",
    "Multi-programme dataset spanning agronomy, design, education, nursing, journalism, management, social service, and technology degrees. Combines enrollment data with academic performance through first two semesters to enable early dropout prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863667ed",
   "metadata": {},
   "source": [
    "## 2.B Dataset Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c05d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset shape\n",
    "shape = df_dataset.shape\n",
    "tools.print_message('success', 'Dataset shape', format_dict={'rows': f\"{shape[0]:,}\", 'columns': f\"{shape[1]:,}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9703e8",
   "metadata": {},
   "source": [
    "   ## 2.C Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbceda",
   "metadata": {},
   "source": [
    "## 2.D Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise column names, remove spaces, and convert to snake_case, replace special characters\n",
    "def normalise_columns(df):\n",
    "    def snake_case(text):\n",
    "        text = '_'.join(' '.join(str(text).split()).lower().split())\n",
    "        text = text.replace('/', '_')\n",
    "        text = text.replace('(', '')\n",
    "        text = text.replace(')', '')\n",
    "        text = text.replace('\\'', '')\n",
    "        return '_'.join(' '.join(str(text).split()).lower().split())\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [snake_case(col) for col in df.columns]\n",
    "    return df\n",
    "df_dataset = normalise_columns(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e20bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_dataset.columns.tolist()\n",
    "tools.print_message('info', 'Features in dataset', format_dict={'number of features': len(features) - 1})\n",
    "print(\"Features:\\n\" + \",\\n \".join(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b5a85",
   "metadata": {},
   "source": [
    "#### Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = config.get('dataset_features', {}).get('continuous_features', [])\n",
    "all_present = all(feature in df_dataset.columns for feature in continuous_features)\n",
    "if not all_present or continuous_features == []:\n",
    "    raise ValueError(\"Continuous features not found in dataset or not defined in config.toml\")\n",
    "tools.print_message('info', 'Continuous features', format_dict={'number of continuous features': len(continuous_features), 'all present': all_present})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeee18a",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = config.get('dataset_features', {}).get('categorical_features', [])\n",
    "all_present = all(feature in df_dataset.columns for feature in categorical_features)\n",
    "if not all_present or categorical_features == []:\n",
    "    raise ValueError(f\"Categorical features not found in dataset or not defined in config.toml\")\n",
    "tools.print_message('info', 'Categorical features', format_dict={'number of categorical features': len(categorical_features), 'all present': all_present})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the spelling of 'nacionality' to english 'nationality'\n",
    "if 'nacionality' in df_dataset.columns:\n",
    "    df_dataset.rename(columns={'nacionality': 'nationality'}, inplace=True)\n",
    "if 'nacionality' in categorical_features:\n",
    "    categorical_features.remove('nacionality')\n",
    "    categorical_features.append('nationality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8901eb",
   "metadata": {},
   "source": [
    "## 2.E Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b1d91",
   "metadata": {},
   "source": [
    "# 3. Target Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2344b50",
   "metadata": {},
   "source": [
    "## 3.A Summary\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Target Variable Transformation**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Binary Classification Approach**</span>\n",
    "\n",
    "**Transformation**: Combined \"Graduate\" and \"Enrolled\" into single \"Continuation\" class (1), keeping \"Dropout\" as \"Withdrawn\" (0).\n",
    "\n",
    "**Rationale**: From an institutional perspective, both graduates and currently enrolled students represent successful outcomes. The critical distinction is identifying students at risk of withdrawal for early intervention.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Algorithmic Benefits**</span>\n",
    "\n",
    "**Logistic Regression**:\n",
    "- Natural binary classification design\n",
    "- **Clear probability interpretation**: Outputs values between 0-1 representing the probability a student will continue (e.g., 0.75 = 75% chance of continuation, 0.25 = 25% chance of withdrawal)\n",
    "- Avoids multi-class complexity\n",
    "\n",
    "**k-Nearest Neighbours**:\n",
    "- Eliminates voting ties between three classes\n",
    "- Cleaner decision boundaries in feature space\n",
    "- Simplified majority voting mechanism\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Class Distribution Results**</span>\n",
    "\n",
    "| **Original (3-class)** | **Binary (2-class)** |\n",
    "|------------------------|----------------------|\n",
    "| Graduate: 2,209 (49.9%) | Continuation: 3,003 (67.9%) |\n",
    "| Enrolled: 794 (17.9%) | Withdrawn: 1,421 (32.1%) |\n",
    "| Dropout: 1,421 (32.1%) | |\n",
    "\n",
    "**Outcome**: Manageable 68:32 class ratio suitable for both algorithms without requiring complex balancing techniques.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Practical Impact**</span>\n",
    "This transformation aligns the machine learning task with institutional goals: identifying students who need support to continue their studies, regardless of their current academic standing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659287d",
   "metadata": {},
   "source": [
    "## 3.B Target Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb06e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new target column with renamed values for one vs rest classification\n",
    "df_dataset['target_binary'] = df_dataset['target'].map({'Dropout': 0, 'Graduate': 1, 'Enrolled': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1981be",
   "metadata": {},
   "source": [
    "## 3.C Target Variable Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df_dataset):\n",
    "    # Create 1x2 subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Total count for percentage calculation\n",
    "    total = len(df_dataset)\n",
    "\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "    # First subplot - Target distribution\n",
    "    ax1 = sns.countplot(data=df_dataset, x='target', ax=axes[0], color=colors[0])\n",
    "    axes[0].set_title('Target Distribution')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].set_axisbelow(True)\n",
    "\n",
    "    # Add data labels on bars for Target\n",
    "    for p in ax1.patches:\n",
    "        count = int(p.get_height())\n",
    "        percentage = (count / total) * 100\n",
    "        ax1.annotate(f'{count:,} ({percentage:.1f}%)', \n",
    "                    (p.get_x() + p.get_width()/2., p.get_height()), \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    # Second subplot - Target_OVR distribution\n",
    "    ax2 = sns.countplot(data=df_dataset, x='target_binary', ax=axes[1], color=colors[1])\n",
    "    axes[1].set_title('Target Distribution (Binary)')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].set_axisbelow(True)\n",
    "\n",
    "    # Add data labels on bars for Target_Binary\n",
    "    for p in ax2.patches:\n",
    "        count = int(p.get_height())\n",
    "        percentage = (count / total) * 100\n",
    "        ax2.annotate(f'{count:,} ({percentage:.1f}%)', \n",
    "                    (p.get_x() + p.get_width()/2., p.get_height()), \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    # Add horizontal legend below the plots\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='grey', label='0 = Withdrawn'),\n",
    "                      Patch(facecolor='grey', label='1 = Continuation')]\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_copy = df_dataset.copy()\n",
    "plot_target_distribution(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831eae74",
   "metadata": {},
   "source": [
    "# 4. Continuous Features Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9c08f",
   "metadata": {},
   "source": [
    "## 4.A Summary\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Continuous Features Analysis Summary**</span>\n",
    "\n",
    "This analysis examines 18 continuous features in the student dataset to understand their distributions, relationships, and predictive power for identifying student withdrawal risk.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. Feature Distributions**</span>\n",
    "\n",
    "The histograms reveal several important patterns:\n",
    "- **Academic grades** (qualification and admission grades) follow roughly normal distributions around 120-130\n",
    "- **Age at enrollment** is heavily right-skewed with most students aged 17-25\n",
    "- **Curricular units** (credited, enrolled, approved) show highly skewed distributions with many zero values\n",
    "- **Economic indicators** (unemployment, inflation, GDP) show varied patterns across different time periods\n",
    "\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Multicollinearity Issues**</span>\n",
    "\n",
    "**Multicollinearity** occurs when features are highly correlated with each other, meaning they provide similar information to the model.\n",
    "\n",
    "**High Correlations (>0.8):**\n",
    "- 1st and 2nd semester grades: 0.84\n",
    "- 1st and 2nd semester credited units: 0.94  \n",
    "- 1st and 2nd semester enrolled units: 0.94\n",
    "- 1st and 2nd semester approved units: 0.90\n",
    "\n",
    "**VIF Analysis Results:**\n",
    "**VIF (Variance Inflation Factor)** measures how much a feature's variance increases due to correlation with other features. Values >5 indicate problematic multicollinearity.\n",
    "\n",
    "Several features show problematic multicollinearity:\n",
    "- Enrolled units (1st sem): VIF = 23.49 (HIGH)\n",
    "- Credited units (1st sem): VIF = 15.57 (HIGH)  \n",
    "- Enrolled units (2nd sem): VIF = 16.42 (HIGH)\n",
    "- Other academic performance metrics: VIF = 5-12 (MODERATE-HIGH)\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Predictive Power**</span>\n",
    "\n",
    "**Point-biserial correlation** measures the relationship between a continuous variable and a binary variable (in this case, withdrawal vs continuation). Values range from -1 to +1, with stronger correlations indicating better predictive power.\n",
    "\n",
    "**Strongest predictors** (correlation with target):\n",
    "- 2nd semester grades: 0.57\n",
    "- 2nd semester approved units: 0.57\n",
    "- 1st semester grades: 0.48\n",
    "- 1st semester approved units: 0.48\n",
    "\n",
    "**Moderate predictors:**\n",
    "- Age at enrollment: -0.25 (negative correlation - older students more likely to drop out)\n",
    "- Units enrolled/evaluated: 0.12-0.16\n",
    "\n",
    "**Weak predictors:**\n",
    "- Economic indicators: -0.03 to 0.05\n",
    "- Previous qualifications: 0.08-0.10\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Implications for Machine Learning Models**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**k-Nearest Neighbours (k-NN)**</span>\n",
    "- **Distance calculation impact**: Multicollinear features will dominate distance measurements, reducing model effectiveness\n",
    "- **Scaling required**: **Scaling** transforms features to similar ranges (e.g., 0-1) so no single feature dominates distance calculations due to its scale. Features have vastly different scales (grades 0-190 vs units 0-26)\n",
    "- **Curse of dimensionality**: The **curse of dimensionality** means that as the number of features increases, data points become increasingly sparse and distant from each other, making similarity measures less meaningful. 18 features may be too many without dimensionality reduction\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Logistic Regression**</span>\n",
    "- **Coefficient instability**: **Coefficient instability** occurs when small changes in data cause large changes in model coefficients, making the model unreliable. High multicollinearity will make coefficients unreliable and difficult to interpret\n",
    "- **Convergence issues**: Redundant features may cause numerical instability during fitting\n",
    "- **Feature redundancy**: Similar information captured multiple times reduces model efficiency\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Recommended Actions**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. Feature Selection**</span>\n",
    "- **Remove redundant features**: Keep only one semester's academic metrics (2nd semester shows higher correlation)\n",
    "- **Drop weak predictors**: Consider removing economic indicators and previous qualification grades\n",
    "- **Priority features**: Focus on 2nd semester grades, approved units, age, and enrollment patterns\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Feature Engineering**</span>\n",
    "- **Combine related features**: Create composite academic performance scores\n",
    "- **Feature scaling**: Apply StandardScaler or MinMaxScaler for k-NN\n",
    "- **Handle skewness**: Consider log transformation for highly skewed unit counts\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Dimensionality Reduction**</span>\n",
    "- **Principal Component Analysis (PCA)**: **PCA** creates new features that are combinations of original features, capturing the most important patterns while reducing the total number of features. Reduce academic performance features to key components\n",
    "- **Correlation-based filtering**: Remove features with correlation >0.8\n",
    "- **Univariate selection**: **Univariate selection** evaluates each feature individually against the target variable and keeps only the most statistically significant ones. Keep only features with significant correlation to target (p<0.05)\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**4. Model-Specific Preparations**</span>\n",
    "- **For k-NN**: Mandatory feature scaling, consider feature selection to reduce noise\n",
    "- **For Logistic Regression**: Address multicollinearity first, then apply **regularisation (L1/L2)** if needed. **Regularisation** adds a penalty term to the model to prevent overfitting: L1 (Lasso) can automatically remove unimportant features by setting their coefficients to zero, whilst L2 (Ridge) shrinks coefficients towards zero to reduce their impact.\n",
    "\n",
    "This analysis reveals that academic performance metrics are the strongest predictors, but careful feature selection is essential to avoid multicollinearity issues that could harm both models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969ffaf",
   "metadata": {},
   "source": [
    "## 4.B Continuous Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous_features_distribution(df_dataset, continuous_features):\n",
    "    \"\"\"\n",
    "    Create a visualisation of continuous features distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_dataset : pandas.DataFrame\n",
    "        DataFrame containing the dataset with continuous features\n",
    "    continuous_features : list\n",
    "        List of continuous feature column names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set style for presentation\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Calculate grid size based on number of features\n",
    "    n_features = len(continuous_features)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Create the visualisation\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    fig.suptitle('Continuous Features Distribution Analysis', fontsize=24, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Flatten axes for easier indexing (handle single row case)\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Colors for different feature types (extended palette)\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c', '#e67e22', '#34495e',\n",
    "              '#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dda0dd', '#98d8c8', '#f7dc6f',\n",
    "              '#bb8fce', '#85c1e9'] * 2  # Repeat pattern if needed\n",
    "    \n",
    "    for i, feature in enumerate(continuous_features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create histogram\n",
    "        data = df_dataset[feature]\n",
    "        \n",
    "        # Histogram\n",
    "        ax.hist(data, bins=30, alpha=0.7, color=colors[i % len(colors)], edgecolor='white', linewidth=0.5)\n",
    "        \n",
    "        # Add statistics text box\n",
    "        stats_text = f'Mean: {data.mean():.1f}\\nStd: {data.std():.1f}\\nRange: {data.min():.1f} - {data.max():.1f}'\n",
    "        ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', horizontalalignment='right', \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_title(feature, fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.set_xlabel('Value', fontsize=11)\n",
    "        ax.set_ylabel('Frequency', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Hide unused subplots if fewer features than grid spaces\n",
    "    total_subplots = n_rows * n_cols\n",
    "    for j in range(len(continuous_features), total_subplots):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    # Adjust layout with more padding from title\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Use the continuous_features already defined in scope\n",
    "# Create the plot\n",
    "df_copy = df_dataset.copy()\n",
    "fig = plot_continuous_features_distribution(df_copy, continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a19c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter index for min, max and mean values\n",
    "df_dataset[continuous_features].agg(['min', 'max', 'mean']).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb8dd3",
   "metadata": {},
   "source": [
    "## 4.C Continuous Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a31343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df_dataset, continuous_features):\n",
    "    \"\"\"Creates correlation heatmap for continuous variables\"\"\"\n",
    "    \n",
    "    correlation_matrix = df_dataset[continuous_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True,\n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f')\n",
    "    \n",
    "    plt.title('Correlation Matrix - Continuous Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "correlation_matrix = plot_correlation_heatmap(df_dataset, continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ea172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_high_correlations(df_dataset, continuous_features, threshold=0.8):\n",
    "    \"\"\"Find highly correlated pairs above threshold\"\"\"\n",
    "\n",
    "    correlation_matrix = df_dataset[continuous_features].corr()\n",
    "    \n",
    "    print(f\"Highly correlated pairs (>{threshold}):\")    \n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > threshold:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                high_corr_pairs.append((col1, col2, corr_value))\n",
    "                tools.print_message('attention', f\"{col1} and {col2}: {corr_value:.2f}\")\n",
    "    \n",
    "    if not high_corr_pairs:\n",
    "        print(f\"No pairs with correlation > {threshold} found\")\n",
    "    \n",
    "    return high_corr_pairs\n",
    "high_corrs = find_high_correlations(df_dataset, continuous_features, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed181c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vif(df_dataset, continuous_features):\n",
    "    \"\"\" Check Variance Inflation Factor (VIF) for multicollinearity in continuous features.\"\"\"\n",
    "    print('VIF Analysis:')\n",
    "\n",
    "    # Add constant and calculate VIF\n",
    "    vif_data = add_constant(df_dataset[continuous_features].dropna())\n",
    "    \n",
    "    problematic_features = []\n",
    "    for i in range(1, vif_data.shape[1]):  # Skip constant column\n",
    "        col = continuous_features[i-1]\n",
    "        vif = variance_inflation_factor(vif_data.values, i)\n",
    "        if vif > 5:\n",
    "            problematic_features.append((col, vif))\n",
    "            level = 'HIGH' if vif > 10 else 'MODERATE'\n",
    "            tools.print_message('attention', f\"{col}: {vif:.2f} ({level})\")\n",
    "    \n",
    "    if not problematic_features:\n",
    "        tools.print_message('success', 'No multicollinearity issues found (VIF <= 5)')\n",
    "    \n",
    "    return problematic_features\n",
    "\n",
    "problematic_vif = check_vif(df_dataset, continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc258f8",
   "metadata": {},
   "source": [
    "## 4.D Continuous Feature Target Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_bar_chart(df_dataset, continuous_features, target_col='target_binary'):\n",
    "    colors = ['#3498db', '#e74c3c']  # Blue for positive, Red for negative\n",
    "\n",
    "    if target_col not in df_dataset.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in dataset\")\n",
    "\n",
    "    correlations = {}\n",
    "    p_values = {}\n",
    "\n",
    "    for col in continuous_features:\n",
    "        if col in df_dataset.columns:\n",
    "            target_data = pd.to_numeric(df_dataset[target_col], errors='coerce')\n",
    "            feature_data = pd.to_numeric(df_dataset[col], errors='coerce')\n",
    "            valid_mask = ~(pd.isna(target_data) | pd.isna(feature_data))\n",
    "\n",
    "            if valid_mask.sum() >= 2:\n",
    "                corr, p_val = pointbiserialr(target_data[valid_mask], feature_data[valid_mask])\n",
    "                correlations[col] = float(round(corr, 4))\n",
    "                p_values[col] = float(round(p_val, 6))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(correlations)), list(correlations.values()))\n",
    "\n",
    "    for i, (bar, p_val) in enumerate(zip(bars, p_values.values())):\n",
    "        corr_val = correlations[list(correlations.keys())[i]]\n",
    "        if p_val < 0.05:\n",
    "            bar.set_color(colors[0] if corr_val > 0 else colors[1])\n",
    "        else:\n",
    "            bar.set_color('lightgray')\n",
    "\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.gca().set_axisbelow(True)\n",
    "\n",
    "    for i, (bar, corr_val) in enumerate(zip(bars, correlations.values())):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height > 0 else -0.03),\n",
    "                 f'{corr_val:.3f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                 fontsize=9, fontweight='bold')\n",
    "\n",
    "    if correlations:\n",
    "        y_min = min(correlations.values())\n",
    "        y_max = max(correlations.values())\n",
    "        y_range = y_max - y_min\n",
    "        plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xticks(range(len(correlations)), list(correlations.keys()), rotation=45, ha='right')\n",
    "    plt.ylabel('Point-Biserial Correlation')\n",
    "    plt.title('Point-Biserial Correlations with Target')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {'correlations': correlations, 'p_values': p_values}\n",
    "\n",
    "correlations = plot_correlation_bar_chart(df_dataset, continuous_features)\n",
    "print('Correlations with target:', correlations['correlations'])\n",
    "print('P-values:', correlations['p_values'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d1f82",
   "metadata": {},
   "source": [
    "# 5. Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b35073",
   "metadata": {},
   "source": [
    "## 5.A Summary\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Categorical Features Analysis Summary**</span>\n",
    "\n",
    "This analysis examines 18 categorical features in the student dataset to understand their diversity, balance, and predictive value for identifying student withdrawal risk.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. Feature Cardinality**</span>\n",
    "\n",
    "**Cardinality** refers to the number of unique categories within each feature. The analysis reveals:\n",
    "- **Binary features** (8 features): Simple yes/no or male/female categories with 2 values each\n",
    "- **Low cardinality** (2 features): Marital status (6), application order (8) \n",
    "- **Medium cardinality** (4 features): Course (17), previous qualification (17), application mode (18), nationality (21)\n",
    "- **High cardinality** (4 features): Parents' qualifications and occupations (29-46 categories)\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Class Imbalance Issues**</span>\n",
    "\n",
    "**Class imbalance** occurs when one category dominates a feature, making it less useful for prediction.\n",
    "\n",
    "**Severely imbalanced features** (>95% in one category):\n",
    "- Nationality: 97.51% Portuguese students\n",
    "- Educational special needs: 98.85% have no special needs\n",
    "- International status: 97.51% are domestic students\n",
    "\n",
    "These features provide little variation and minimal predictive value.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Statistical Significance**</span>\n",
    "\n",
    "**Chi-square tests** measure whether categorical features are statistically associated with the target variable. A p-value <0.05 indicates significant association.\n",
    "\n",
    "**Highly significant predictors** (p<0.001):\n",
    "- Tuition fees up to date: χ² = 811.93\n",
    "- Application mode: χ² = 399.12\n",
    "- Scholarship holder: χ² = 265.10\n",
    "- Course type: χ² = 298.27\n",
    "\n",
    "**Non-significant predictors** (p>0.05):\n",
    "- International status\n",
    "- Nationality  \n",
    "- Educational special needs\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**4. Information Content**</span>\n",
    "\n",
    "**Mutual information** measures how much information each feature provides about the target variable. Higher scores indicate more predictive power.\n",
    "\n",
    "**Most informative features:**\n",
    "- Tuition fees up to date: 0.085\n",
    "- Scholarship holder: 0.047\n",
    "- Course type: 0.033\n",
    "- Application mode: 0.029\n",
    "\n",
    "**Uninformative features** (MI = 0.00):\n",
    "- Nationality, daytime/evening attendance, displaced status, international status\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Implications for Machine Learning Models**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**k-Nearest Neighbours (k-NN)**</span>\n",
    "- **Encoding requirements**: **One-hot encoding** creates binary dummy variables for each category (e.g., \"Course_Design\"=1, \"Course_Nursing\"=0). High cardinality features create many new dimensions\n",
    "- **Curse of dimensionality**: Parents' occupations (32-46 categories) would create 100+ new features after encoding\n",
    "- **Distance distortion**: Irrelevant categories can dominate distance calculations\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Logistic Regression**</span>\n",
    "- **Coefficient interpretation**: Each category gets its own coefficient after encoding\n",
    "- **Overfitting risk**: High cardinality features create many parameters that may not generalise well\n",
    "- **Multicollinearity**: **Label encoding** assigns numbers to categories (e.g., Course_1, Course_2), but this implies false ordering relationships\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Recommended Actions**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. Remove Uninformative Features**</span>\n",
    "- **Drop severely imbalanced**: Nationality, educational special needs, international status\n",
    "- **Remove zero-information**: Features with MI score = 0.00\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Handle High Cardinality**</span>\n",
    "- **Grouping strategy**: Combine rare categories in parents' occupations into \"Other\" category\n",
    "- **Target encoding**: **Target encoding** replaces categories with their average target value, reducing dimensions whilst preserving predictive power\n",
    "- **Feature selection**: Keep only top categories by frequency\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Encoding Strategy**</span>\n",
    "- **For k-NN**: Use one-hot encoding for low cardinality features (<10 categories)\n",
    "- **For Logistic Regression**: One-hot encoding with **regularisation** to handle multiple coefficients. **Regularisation** prevents overfitting by penalising large coefficients\n",
    "- **Alternative**: Label encoding for ordinal features (application order, qualifications)\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**4. Priority Features**</span>\n",
    "**Keep these high-value features:**\n",
    "- Tuition fees up to date\n",
    "- Scholarship holder  \n",
    "- Course type\n",
    "- Application mode\n",
    "- Gender\n",
    "- Debtor status\n",
    "\n",
    "**Consider removing:**\n",
    "- Parents' detailed occupations/qualifications (too high cardinality)\n",
    "- Nationality, international status (too imbalanced)\n",
    "- Educational special needs (no variation)\n",
    "\n",
    "This analysis shows that financial and course-related factors are the strongest categorical predictors, whilst demographic details provide limited additional value and risk model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cardinality(df, cols):\n",
    "    print(df)\n",
    "    \"\"\"Return DataFrame with cardinality info\"\"\"\n",
    "    cardinality_data = []\n",
    "    for col in cols:\n",
    "        cardinality_data.append({\n",
    "            'Feature': col,\n",
    "            'Cardinality': df[col].nunique()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(cardinality_data).sort_values('Cardinality')\n",
    "\n",
    "# Usage:\n",
    "cardinality_df = check_cardinality(df_dataset, categorical_features)\n",
    "cardinality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f84926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_imbalance(df, cols, threshold=0.95):\n",
    "    \"\"\"Find features where one category dominates (>95% of data)\"\"\"\n",
    "    imbalanced_features = []\n",
    "    \n",
    "    for col in cols:\n",
    "        max_proportion = df[col].value_counts(normalize=True).max()\n",
    "        if max_proportion > threshold:\n",
    "            imbalanced_features.append({\n",
    "                'Feature': col,\n",
    "                'Max_Proportion': f'{max_proportion:.2%}',\n",
    "                'Dominant_Category': df[col].value_counts().index[0]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(imbalanced_features)\n",
    "\n",
    "# Features with >95% in one category are usually useless\n",
    "imbalanced = check_class_imbalance(df_dataset, categorical_features)\n",
    "imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df, categorical_features, target_col):\n",
    "    \"\"\"Test which categorical features are significantly associated with target\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        contingency_table = pd.crosstab(df[col], df[target_col])\n",
    "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': col,\n",
    "            'Chi2_Score': chi2,\n",
    "            'P_Value': p_value,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('P_Value')\n",
    "\n",
    "# Features with high p-values (>0.05) have weak association with target\n",
    "chi2_results = chi_square_test(df_dataset, categorical_features, 'target_binary')\n",
    "chi2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_analysis(df, categorical_features, target_col):\n",
    "    \"\"\"Calculate mutual information between categorical features and target\"\"\"\n",
    "    # Label encode all categorical features for mutual info\n",
    "    encoded_data = df[categorical_features].copy()\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        encoded_data[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_scores = mutual_info_classif(encoded_data, df[target_col])\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Feature': categorical_features,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Higher MI scores = more informative features\n",
    "mi_results = mutual_info_analysis(df_dataset, categorical_features, 'target_binary')\n",
    "mi_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4c030",
   "metadata": {},
   "source": [
    "# 6. Outlier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66486b4f",
   "metadata": {},
   "source": [
    "## 6.A Summary\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Outlier Analysis Summary**</span>\n",
    "\n",
    "This analysis examines extreme values in the continuous features to understand their nature, frequency, and potential impact on our machine learning models for predicting student withdrawal.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. What Are Outliers and Why Do They Matter?**</span>\n",
    "\n",
    "**Outliers** are data points that fall significantly outside the normal range of values for a feature. They can arise from:\n",
    "- **Data entry errors** (typos, measurement mistakes)\n",
    "- **Legitimate extreme cases** (mature students, exceptional performance)\n",
    "- **Missing or incomplete data** (zeros from non-attendance)\n",
    "\n",
    "**Why outlier analysis matters:**\n",
    "- Outliers can **skew model performance** and lead to poor predictions\n",
    "- Some outliers contain **valuable information** (e.g., failing students are important for dropout prediction)\n",
    "- Different models are affected differently by extreme values\n",
    "- Proper handling improves **model robustness** and **generalisation**\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Key Findings from Our Analysis**</span>\n",
    "\n",
    "**High Outlier Features (>10%):**\n",
    "- **Age at enrollment**: 10.0% outliers - likely mature students (>25 years)\n",
    "- **1st semester grades**: 16.4% outliers - mostly zeros from non-attending students\n",
    "- **2nd semester grades**: 15.8% outliers - zeros from early dropouts\n",
    "- **Credited/enrolled units**: 9-13% outliers - zeros from incomplete coursework\n",
    "\n",
    "**Academic Performance Pattern:**\n",
    "Most outliers in academic metrics are **zeros**, representing students who:\n",
    "- Didn't attend classes\n",
    "- Failed to complete assessments\n",
    "- Dropped out early in the semester\n",
    "\n",
    "**Economic Indicators:**\n",
    "Very clean data with <1% outliers - no action needed.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Understanding the Context**</span>\n",
    "\n",
    "**Important insight**: Many \"outliers\" in our dataset are **legitimate and informative values** rather than errors. Zero grades and zero credited units are strong indicators of student struggle and potential withdrawal - exactly what we want to predict.\n",
    "\n",
    "**The IQR Method**: We used the **Interquartile Range (IQR) method** to detect outliers, where values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR are flagged as outliers. This is a standard statistical approach that identifies the most extreme 5-10% of values.\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Impact on Machine Learning Models**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**k-Nearest Neighbours (k-NN)**</span>\n",
    "\n",
    "**Distance calculation sensitivity**: k-NN calculates distances between data points to find similar cases. Outliers can:\n",
    "- **Dominate distance calculations** if features aren't scaled properly\n",
    "- **Create isolated points** that don't have meaningful neighbours\n",
    "- **Skew similarity measurements** leading to poor classification\n",
    "\n",
    "**Specific issues in our data:**\n",
    "- Large age outliers (>50) could overwhelm other features in distance calculations\n",
    "- Zero academic performance creates clusters of failing students (which might actually be useful)\n",
    "- **Feature scaling becomes critical** to prevent grade outliers (0-200 range) from dominating\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**Logistic Regression**</span>\n",
    "\n",
    "**Coefficient estimation problems**: Logistic regression finds the best linear boundary between classes. Outliers can:\n",
    "- **Pull the decision boundary** towards extreme values\n",
    "- **Inflate coefficient estimates** making the model unstable\n",
    "- **Reduce model interpretability** by giving extreme values too much influence\n",
    "\n",
    "**Leverage and influence**: **Leverage** measures how far a data point is from others in feature space, whilst **influence** measures how much a single point affects the model. High-leverage outliers can have disproportionate influence on the final model.\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Recommended Actions**</span>\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**1. Keep Informative Outliers**</span>\n",
    "**DO NOT remove academic performance outliers** - they're crucial for prediction:\n",
    "- Zero grades indicate struggling students\n",
    "- Zero units show non-engagement\n",
    "- These are **strong predictors** of withdrawal risk\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**2. Address Problematic Outliers**</span>\n",
    "**Age at enrollment**: Consider capping extreme ages (>60) as they might represent data errors or very unusual cases that could skew the model.\n",
    "\n",
    "**Previous qualification grades**: Very low scores (<50) might indicate data entry errors and could be investigated.\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**3. Feature Engineering Opportunities**</span>\n",
    "Instead of removing outliers, create **binary indicator features**:\n",
    "- `has_zero_grade_sem1`: Student received zero grade in first semester\n",
    "- `has_zero_units_sem1`: Student completed zero units in first semester  \n",
    "- `mature_student`: Student aged >25 at enrollment\n",
    "- `low_prior_qualification`: Previous qualification grade <100\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**4. Model-Specific Preparations**</span>\n",
    "\n",
    "**For k-NN:**\n",
    "- **Mandatory feature scaling** to prevent outliers from dominating distance calculations\n",
    "- Consider **robust scaling** (median and IQR-based) instead of standard scaling\n",
    "- Keep academic outliers as they represent meaningful patterns\n",
    "\n",
    "**For Logistic Regression:**\n",
    "- **Feature scaling** recommended but less critical than for k-NN\n",
    "- Consider **robust regression techniques** if outliers prove problematic\n",
    "- **Regularisation** (L1/L2) can help reduce outlier influence automatically\n",
    "\n",
    "### <span style=\"color: #2E86AB;\">**5. Validation Strategy**</span>\n",
    "**Monitor model performance** with and without outlier treatment to ensure we're not removing valuable predictive information. Academic performance outliers likely **improve** rather than harm prediction accuracy.\n",
    "\n",
    "### <span style=\"color: #e74c3c;\">**Next Steps**</span>\n",
    "\n",
    "With outlier analysis complete, proceed to:\n",
    "1. **Feature selection** - remove redundant and weak predictors identified earlier\n",
    "2. **Feature scaling** - prepare data for both models  \n",
    "3. **Train/test split** - ensure proper data separation\n",
    "4. **Baseline model training** - test both approaches with cleaned data\n",
    "\n",
    "This analysis confirms that most \"outliers\" in our dataset represent legitimate academic struggles - exactly the patterns our models need to learn for accurate dropout prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_analysis(df, continuous_features, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Create comprehensive box plots for outlier analysis of continuous features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset containing the features\n",
    "    continuous_features : list\n",
    "        List of continuous feature column names\n",
    "    figsize : tuple\n",
    "        Figure size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    outlier_summary : dict\n",
    "        Summary of outliers detected for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_features = len(continuous_features)\n",
    "    n_cols = 4  # 4 plots per row\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    fig.suptitle('Outlier Analysis: Box Plots for Continuous Features', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for i, feature in enumerate(continuous_features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create box plot\n",
    "        box_plot = ax.boxplot(df[feature].dropna(), \n",
    "                             patch_artist=True, \n",
    "                             boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                             medianprops=dict(color='red', linewidth=2),\n",
    "                             whiskerprops=dict(color='black', linewidth=1.5),\n",
    "                             capprops=dict(color='black', linewidth=1.5),\n",
    "                             flierprops=dict(marker='o', markerfacecolor='red', \n",
    "                                           markersize=4, alpha=0.6))\n",
    "        \n",
    "        # Calculate outliers using IQR method\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)][feature]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        # Store outlier info\n",
    "        outlier_summary[feature] = {\n",
    "            'count': outlier_count,\n",
    "            'percentage': outlier_percentage,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_values': outliers.tolist()\n",
    "        }\n",
    "        \n",
    "        # Set title with outlier info\n",
    "        ax.set_title(f'{feature}\\n({outlier_count} outliers, {outlier_percentage:.1f}%)', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotate x-axis labels if needed\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for feature, info in outlier_summary.items():\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Outlier_Count': info['count'],\n",
    "            'Outlier_Percentage': round(info['percentage'], 1),\n",
    "            'Lower_Bound': round(info['lower_bound'], 2),\n",
    "            'Upper_Bound': round(info['upper_bound'], 2),\n",
    "            'Sample_Outliers': str([round(x, 2) for x in info['outlier_values'][:5]]) if info['count'] > 0 else 'None'\n",
    "        })\n",
    "    \n",
    "    df_outliers = pd.DataFrame(summary_data)\n",
    "    df_outliers = df_outliers.sort_values('Outlier_Percentage', ascending=False)\n",
    "    \n",
    "    # Print concise summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OUTLIER ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total features analysed: {len(continuous_features)}\")\n",
    "    print(f\"Features with >10% outliers: {len(df_outliers[df_outliers['Outlier_Percentage'] > 10])}\")\n",
    "    print(f\"Features with >5% outliers: {len(df_outliers[df_outliers['Outlier_Percentage'] > 5])}\")\n",
    "    print(\"\\nTop 5 features by outlier percentage:\")\n",
    "    print(df_outliers[['Feature', 'Outlier_Count', 'Outlier_Percentage']].head().to_string(index=False))\n",
    "    \n",
    "    return df_outliers, outlier_summary\n",
    "\n",
    "outlier_info = plot_outlier_analysis(df_dataset, continuous_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
